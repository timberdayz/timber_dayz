#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨å…¥åº“APIè·¯ç”±

v4.5.0æ–°å¢ï¼š
- æ²»ç†ç»Ÿè®¡APIï¼ˆ3ä¸ªï¼‰
- è‡ªåŠ¨å…¥åº“APIï¼ˆ3ä¸ªï¼‰
- å¤ç”¨ç°æœ‰çš„progress_tracker
"""

from fastapi import APIRouter, Depends, Query, HTTPException
from sqlalchemy.orm import Session
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
from datetime import datetime

from backend.models.database import get_db, get_async_db
from backend.services.governance_stats import get_governance_stats
from backend.services.auto_ingest_orchestrator import get_auto_ingest_orchestrator
from backend.services.progress_tracker import progress_tracker
from backend.utils.api_response import success_response, error_response
from backend.utils.error_codes import ErrorCode, get_error_type
from modules.core.logger import get_logger

logger = get_logger(__name__)
router = APIRouter()

# ==================== è¯·æ±‚æ¨¡å‹ ====================

class BatchAutoIngestRequest(BaseModel):
    """æ‰¹é‡è‡ªåŠ¨å…¥åº“è¯·æ±‚"""
    platform: Optional[str] = Field(
        None, description="å¹³å°ä»£ç ï¼›ä¼ å…¥'*'æˆ–çœç•¥è¡¨ç¤ºå…¨éƒ¨å¹³å°"
    )
    domains: Optional[List[str]] = Field(None, description="æ•°æ®åŸŸåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œç©º=å…¨éƒ¨ï¼‰")
    granularities: Optional[List[str]] = Field(None, description="ç²’åº¦åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œç©º=å…¨éƒ¨ï¼‰")
    since_hours: Optional[int] = Field(None, description="åªå¤„ç†æœ€è¿‘Nå°æ—¶çš„æ–‡ä»¶")
    limit: int = Field(100, description="æœ€å¤šå¤„ç†Nä¸ªæ–‡ä»¶", ge=1, le=1000)
    only_with_template: bool = Field(True, description="åªå¤„ç†æœ‰æ¨¡æ¿çš„æ–‡ä»¶")
    allow_quarantine: bool = Field(True, description="å…è®¸éš”ç¦»é”™è¯¯æ•°æ®")


class SingleAutoIngestRequest(BaseModel):
    """å•æ–‡ä»¶è‡ªåŠ¨å…¥åº“è¯·æ±‚"""
    file_id: int = Field(..., description="æ–‡ä»¶ID")
    only_with_template: bool = Field(True, description="åªå¤„ç†æœ‰æ¨¡æ¿çš„æ–‡ä»¶")
    allow_quarantine: bool = Field(True, description="å…è®¸éš”ç¦»é”™è¯¯æ•°æ®")


# ==================== æ²»ç†ç»Ÿè®¡API ====================

@router.get("/governance/overview")
async def get_governance_overview(
    platform: Optional[str] = Query(None, description="å¹³å°ä»£ç "),
    db: AsyncSession = Depends(get_async_db)
):
    """
    è·å–æ²»ç†æ¦‚è§ˆç»Ÿè®¡
    
    è¿”å›ï¼š
    - pending_files: å¾…å…¥åº“æ–‡ä»¶æ•°
    - template_coverage: æ¨¡æ¿è¦†ç›–åº¦ï¼ˆ%ï¼‰
    - today_auto_ingested: ä»Šæ—¥è‡ªåŠ¨å…¥åº“æ•°
    - missing_templates_count: ç¼ºå°‘æ¨¡æ¿çš„åŸŸÃ—ç²’åº¦æ•°
    """
    try:
        stats_service = get_governance_stats(db)
        overview = stats_service.get_overview(platform)
        
        return success_response(data=overview)
    except Exception as e:
        logger.error(f"[API] è·å–æ²»ç†æ¦‚è§ˆå¤±è´¥: {e}", exc_info=True)
        return error_response(
            code=ErrorCode.DATABASE_QUERY_ERROR,
            message="è·å–æ²»ç†æ¦‚è§ˆå¤±è´¥",
            error_type=get_error_type(ErrorCode.DATABASE_QUERY_ERROR),
            detail=str(e),
            status_code=500
        )


@router.get("/governance/missing-templates")
async def get_missing_templates(
    platform: Optional[str] = Query(None, description="å¹³å°ä»£ç "),
    db: AsyncSession = Depends(get_async_db)
):
    """
    è·å–ç¼ºå°‘æ¨¡æ¿çš„åŸŸÃ—ç²’åº¦æ¸…å•
    
    è¿”å›ï¼š
    [{domain, granularity, file_count}]
    """
    try:
        stats_service = get_governance_stats(db)
        missing = stats_service.get_missing_templates(platform)
        
        return {
            "success": True,
            "data": missing
        }
    except Exception as e:
        logger.error(f"[API] è·å–ç¼ºå°‘æ¨¡æ¿æ¸…å•å¤±è´¥: {e}", exc_info=True)
        return error_response(
            code=ErrorCode.DATABASE_QUERY_ERROR,
            message="è·å–ç¼ºå°‘æ¨¡æ¿æ¸…å•å¤±è´¥",
            error_type=get_error_type(ErrorCode.DATABASE_QUERY_ERROR),
            detail=str(e),
            recovery_suggestion="è¯·æ£€æŸ¥æ•°æ®åº“è¿æ¥å’ŒæŸ¥è¯¢å‚æ•°ï¼Œæˆ–è”ç³»ç³»ç»Ÿç®¡ç†å‘˜",
            status_code=500
        )


@router.get("/governance/pending-files")
async def get_pending_files(
    platform: Optional[str] = Query(None, description="å¹³å°ä»£ç "),
    data_domain: Optional[str] = Query(None, description="æ•°æ®åŸŸ"),
    granularity: Optional[str] = Query(None, description="ç²’åº¦"),
    since_hours: Optional[int] = Query(None, description="æœ€è¿‘Nå°æ—¶"),
    limit: int = Query(100, description="è¿”å›æ•°é‡é™åˆ¶", ge=1, le=1000),
    group_by_batch: bool = Query(False, description="æ˜¯å¦æŒ‰æ•°æ®åŸŸ+ç²’åº¦åˆ†ç»„ç»Ÿè®¡"),
    db: AsyncSession = Depends(get_async_db)
):
    """
    è·å–å¾…å…¥åº“æ–‡ä»¶åˆ—è¡¨
    
    v4.11.5æ–°å¢ï¼š
    - group_by_batchå‚æ•°ï¼šæŒ‰æ•°æ®åŸŸ+ç²’åº¦åˆ†ç»„ç»Ÿè®¡æ‰¹æ¬¡ä¿¡æ¯
    
    è¿”å›ï¼š
    [{file_id, file_name, platform, domain, granularity, shop_id, collected_at}]
    æˆ–ï¼ˆå½“group_by_batch=Trueæ—¶ï¼‰ï¼š
    {
        batches: [{domain, granularity, file_count, files: [...]}],
        total_files: int
    }
    """
    try:
        stats_service = get_governance_stats(db)
        files = stats_service.get_pending_files(
            platform=platform,
            data_domain=data_domain,
            granularity=granularity,
            since_hours=since_hours,
            limit=limit
        )
        
        # â­ v4.11.5æ–°å¢ï¼šæŒ‰æ•°æ®åŸŸ+ç²’åº¦åˆ†ç»„ç»Ÿè®¡
        if group_by_batch:
            from collections import defaultdict
            batches = defaultdict(lambda: {'domain': '', 'granularity': '', 'file_count': 0, 'files': []})
            
            for file_info in files:
                domain = file_info.get('domain', 'unknown')
                granularity_val = file_info.get('granularity', 'unknown')
                batch_key = f"{domain}_{granularity_val}"
                
                batches[batch_key]['domain'] = domain
                batches[batch_key]['granularity'] = granularity_val
                batches[batch_key]['file_count'] += 1
                batches[batch_key]['files'].append(file_info)
            
            batch_list = list(batches.values())
            total_files = len(files)
            
            return {
                "success": True,
                "data": {
                    "batches": batch_list,
                    "total_files": total_files,
                    "batch_count": len(batch_list)
                }
            }
        
        return {
            "success": True,
            "data": files
        }
    except Exception as e:
        logger.error(f"[API] è·å–å¾…å…¥åº“æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
        return error_response(
            code=ErrorCode.DATABASE_QUERY_ERROR,
            message="è·å–å¾…å…¥åº“æ–‡ä»¶å¤±è´¥",
            error_type=get_error_type(ErrorCode.DATABASE_QUERY_ERROR),
            detail=str(e),
            recovery_suggestion="è¯·æ£€æŸ¥æ•°æ®åº“è¿æ¥å’ŒæŸ¥è¯¢å‚æ•°ï¼Œæˆ–è”ç³»ç³»ç»Ÿç®¡ç†å‘˜",
            status_code=500
        )


# ==================== è‡ªåŠ¨å…¥åº“API ====================

@router.post("/auto-ingest/single")
async def auto_ingest_single_file(
    request: SingleAutoIngestRequest,
    db: AsyncSession = Depends(get_async_db)
):
    """
    å•æ–‡ä»¶è‡ªåŠ¨å…¥åº“ï¼ˆé‡‡é›†å®Œæˆè§¦å‘ï¼‰
    
    æµç¨‹ï¼š
    1. è·å–æ–‡ä»¶ä¿¡æ¯
    2. æŸ¥æ‰¾æ¨¡æ¿
    3. é¢„è§ˆæ–‡ä»¶
    4. åº”ç”¨æ¨¡æ¿
    5. è°ƒç”¨ingestæ¥å£
    
    è¿”å›ï¼š
    {
        success: bool,
        file_id: int,
        file_name: str,
        status: 'success'|'quarantined'|'failed'|'skipped',
        message: str
    }
    """
    try:
        orchestrator = get_auto_ingest_orchestrator(db)
        # â­ v4.11.6ä¿®å¤ï¼šå•æ–‡ä»¶åŒæ­¥æ—¶ç”Ÿæˆtask_idç”¨äºè¿½è¸ª
        import uuid
        task_id = f"single_file_{request.file_id}_{uuid.uuid4().hex[:8]}"
        
        result = await orchestrator.ingest_single_file(
            file_id=request.file_id,
            only_with_template=request.only_with_template,
            allow_quarantine=request.allow_quarantine,
            task_id=task_id  # â­ v4.11.6ä¿®å¤ï¼šä¼ é€’task_idç”¨äºè¿½è¸ª
        )
        
        # â­ v4.11.6ä¿®å¤ï¼šåœ¨è¿”å›ç»“æœä¸­åŒ…å«task_idï¼Œæ–¹ä¾¿å‰ç«¯è¿½è¸ª
        if result and isinstance(result, dict):
            result['task_id'] = task_id
        
        return result
        
    except Exception as e:
        logger.error(f"[API] å•æ–‡ä»¶è‡ªåŠ¨å…¥åº“å¤±è´¥: {e}", exc_info=True)
        return error_response(
            code=ErrorCode.DATABASE_QUERY_ERROR,
            message="å•æ–‡ä»¶è‡ªåŠ¨å…¥åº“å¤±è´¥",
            error_type=get_error_type(ErrorCode.DATABASE_QUERY_ERROR),
            detail=str(e),
            recovery_suggestion="è¯·æ£€æŸ¥æ•°æ®åº“è¿æ¥å’Œæƒé™ï¼Œæˆ–è”ç³»ç³»ç»Ÿç®¡ç†å‘˜",
            status_code=500
        )


@router.post("/auto-ingest/batch")
async def auto_ingest_batch(
    request: BatchAutoIngestRequest,
    db: AsyncSession = Depends(get_async_db)
):
    """
    æ‰¹é‡è‡ªåŠ¨å…¥åº“ï¼ˆæ‰‹åŠ¨è§¦å‘ï¼‰
    
    æµç¨‹ï¼š
    1. æ‰«æç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶
    2. åˆ›å»ºè¿›åº¦ä»»åŠ¡
    3. é€ä¸ªè°ƒç”¨ingest_single_file
    4. è¿”å›æ±‡æ€»ç»Ÿè®¡
    
    è¿”å›ï¼š
    {
        success: bool,
        task_id: str,
        summary: {
            total_files: int,
            processed: int,
            succeeded: int,
            quarantined: int,
            failed: int,
            skipped_no_template: int
        }
    }
    """
    try:
        orchestrator = get_auto_ingest_orchestrator(db)
        result = await orchestrator.batch_ingest(
            platform=request.platform,
            domains=request.domains,
            granularities=request.granularities,
            since_hours=request.since_hours,
            limit=request.limit,
            only_with_template=request.only_with_template,
            allow_quarantine=request.allow_quarantine
        )
        
        return result
        
    except Exception as e:
        logger.error(f"[API] æ‰¹é‡è‡ªåŠ¨å…¥åº“å¤±è´¥: {e}", exc_info=True)
        return error_response(
            code=ErrorCode.DATABASE_QUERY_ERROR,
            message="æ‰¹é‡è‡ªåŠ¨å…¥åº“å¤±è´¥",
            error_type=get_error_type(ErrorCode.DATABASE_QUERY_ERROR),
            detail=str(e),
            recovery_suggestion="è¯·æ£€æŸ¥æ•°æ®åº“è¿æ¥å’Œæƒé™ï¼Œæˆ–è”ç³»ç³»ç»Ÿç®¡ç†å‘˜",
            status_code=500
        )


@router.get("/auto-ingest/progress/{task_id}")
async def get_auto_ingest_progress(task_id: str):
    """
    æŸ¥è¯¢è‡ªåŠ¨å…¥åº“è¿›åº¦ï¼ˆå¤ç”¨progress_trackerï¼‰
    
    è¿”å›ï¼š
    {
        task_id: str,
        type: 'auto_ingest',
        total: int,
        processed: int,
        succeeded: int,
        quarantined: int,
        failed: int,
        skipped: int,
        status: 'running'|'completed'|'failed',
        percentage: float,
        files: [...]  # æœ€è¿‘å¤„ç†çš„æ–‡ä»¶
    }
    """
    try:
        progress = await progress_tracker.get_task(task_id)
        
        if progress is None:
            return error_response(
                code=ErrorCode.DATA_VALIDATION_FAILED,
                message="ä»»åŠ¡ä¸å­˜åœ¨",
                error_type=get_error_type(ErrorCode.DATA_VALIDATION_FAILED),
                recovery_suggestion="è¯·æ£€æŸ¥ä»»åŠ¡IDæ˜¯å¦æ­£ç¡®ï¼Œæˆ–ç¡®è®¤è¯¥ä»»åŠ¡å·²åˆ›å»º",
                status_code=404
            )
        
        # é€‚é…ProgressTrackerè¿”å›æ ¼å¼åˆ°å‰ç«¯æœŸæœ›æ ¼å¼
        total = progress.get('total_files', progress.get('total', 0))
        processed = progress.get('processed_files', progress.get('processed', 0))
        succeeded = progress.get('valid_rows', progress.get('succeeded', 0))
        quarantined = progress.get('quarantined_rows', progress.get('quarantined', 0))
        failed = progress.get('error_rows', progress.get('failed', 0))
        skipped = progress.get('skipped', 0)
        status = progress.get('status', 'running')
        
        # çŠ¶æ€æ˜ å°„ï¼špending/processing -> running, completed -> completed, failed -> failed
        if status == 'pending' or status == 'processing':
            status = 'running'
        elif status == 'completed':
            status = 'completed'
        elif status == 'failed':
            status = 'failed'
        
        # è®¡ç®—ç™¾åˆ†æ¯”
        percentage = (processed / total * 100) if total > 0 else 0
        
        # ğŸ†• v4.11.5å¢å¼ºï¼šæ·»åŠ å½“å‰æ–‡ä»¶ã€å¤„ç†é˜¶æ®µã€é¢„è®¡æ—¶é—´ã€é”™è¯¯è¯¦æƒ…
        current_file = progress.get('current_file', '')
        current_stage = progress.get('current_stage', '')
        
        # è®¡ç®—é¢„è®¡æ—¶é—´
        start_time_str = progress.get('start_time')
        estimated_time_remaining = None
        if start_time_str and processed > 0 and total > 0:
            try:
                from datetime import datetime
                start_time = datetime.fromisoformat(start_time_str)
                elapsed_seconds = (datetime.now() - start_time).total_seconds()
                avg_time_per_file = elapsed_seconds / processed if processed > 0 else 0
                remaining_files = total - processed
                estimated_seconds = avg_time_per_file * remaining_files
                estimated_time_remaining = round(estimated_seconds, 1)
            except Exception:
                pass
        
        # è·å–é”™è¯¯è¯¦æƒ…
        errors = progress.get('errors', [])
        warnings = progress.get('warnings', [])
        
        # â­ v4.11.5æ–°å¢ï¼šè·å–æ•°æ®è´¨é‡æ£€æŸ¥ç»“æœ
        quality_check = progress.get('quality_check', None)
        
        data = {
                'task_id': task_id,
                'type': progress.get('task_type', 'auto_ingest'),
                'total': total,
                'processed': processed,
                'succeeded': succeeded,
                'quarantined': quarantined,
                'failed': failed,
                'skipped': skipped,
                'status': status,
                'percentage': round(percentage, 1),
                'files': progress.get('files', []),
                # ğŸ†• v4.11.5å¢å¼ºå­—æ®µ
                'current_file': current_file,
                'current_stage': current_stage,
                'estimated_time_remaining': estimated_time_remaining,
                'errors': errors[-10:] if errors else [],  # æœ€è¿‘10ä¸ªé”™è¯¯
                'warnings': warnings[-10:] if warnings else [],  # æœ€è¿‘10ä¸ªè­¦å‘Š
                'start_time': start_time_str,
                'elapsed_seconds': round((datetime.now() - datetime.fromisoformat(start_time_str)).total_seconds(), 1) if start_time_str else None,
                'quality_check': quality_check  # â­ v4.11.5æ–°å¢ï¼šæ•°æ®è´¨é‡æ£€æŸ¥ç»“æœ
            }
        
        return success_response(data=data)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[API] æŸ¥è¯¢è¿›åº¦å¤±è´¥: {e}", exc_info=True)
        return error_response(
            code=ErrorCode.DATABASE_QUERY_ERROR,
            message="æŸ¥è¯¢è¿›åº¦å¤±è´¥",
            error_type=get_error_type(ErrorCode.DATABASE_QUERY_ERROR),
            detail=str(e),
            recovery_suggestion="è¯·æ£€æŸ¥æ•°æ®åº“è¿æ¥å’ŒæŸ¥è¯¢å‚æ•°ï¼Œæˆ–è”ç³»ç³»ç»Ÿç®¡ç†å‘˜",
            status_code=500
        )


@router.get("/auto-ingest/task/{task_id}/logs")
async def get_task_logs(task_id: str, limit: int = Query(50, ge=1, le=200), db: AsyncSession = Depends(get_async_db)):
    """
    è·å–ä»»åŠ¡æ—¥å¿—è¯¦æƒ…ï¼ˆv4.11.4æ–°å¢ï¼‰
    
    è¿”å›ï¼š
    {
        success: bool,
        task_id: str,
        logs: [
            {
                file_id: int,
                file_name: str,
                status: str,
                staged: int,
                imported: int,
                quarantined: int,
                error_message: str,
                duration_seconds: float
            }
        ]
    }
    """
    try:
        from modules.core.db import CatalogFile, StagingOrders, StagingProductMetrics, StagingInventory
        from sqlalchemy import func, text
        
        # æŸ¥è¯¢stagingè¡¨ä¸­çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆæŒ‰task_idï¼‰
        staging_files = []
        
        # ä»staging_ordersæŸ¥è¯¢
        orders_stmt = select(
            StagingOrders.file_id,
            func.count(StagingOrders.id).label('staged_count')
        ).where(
            StagingOrders.ingest_task_id == task_id
        ).group_by(StagingOrders.file_id)
        orders_result = await db.execute(orders_stmt)
        orders_files = orders_result.all()
        
        for file_id, staged_count in orders_files:
            if file_id:
                file_result = await db.execute(select(CatalogFile).where(CatalogFile.id == file_id))
                file_record = file_result.scalar_one_or_none()
                staging_files.append({
                    'file_id': file_id,
                    'file_name': file_record.file_name if file_record else f'æ–‡ä»¶{file_id}',
                    'staged': staged_count,
                    'domain': 'orders'
                })
        
        # ä»staging_product_metricsæŸ¥è¯¢
        metrics_stmt = select(
            StagingProductMetrics.file_id,
            func.count(StagingProductMetrics.id).label('staged_count')
        ).where(
            StagingProductMetrics.ingest_task_id == task_id
        ).group_by(StagingProductMetrics.file_id)
        metrics_result = await db.execute(metrics_stmt)
        metrics_files = metrics_result.all()
        
        for file_id, staged_count in metrics_files:
            if file_id:
                file_result = await db.execute(select(CatalogFile).where(CatalogFile.id == file_id))
                file_record = file_result.scalar_one_or_none()
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = next((f for f in staging_files if f['file_id'] == file_id), None)
                if existing:
                    existing['staged'] += staged_count
                else:
                    staging_files.append({
                        'file_id': file_id,
                        'file_name': file_record.file_name if file_record else f'æ–‡ä»¶{file_id}',
                        'staged': staged_count,
                        'domain': 'products'
                    })
        
        # ä»staging_inventoryæŸ¥è¯¢
        inventory_stmt = select(
            StagingInventory.file_id,
            func.count(StagingInventory.id).label('staged_count')
        ).where(
            StagingInventory.ingest_task_id == task_id
        ).group_by(StagingInventory.file_id)
        inventory_result = await db.execute(inventory_stmt)
        inventory_files = inventory_result.all()
        
        for file_id, staged_count in inventory_files:
            if file_id:
                file_result = await db.execute(select(CatalogFile).where(CatalogFile.id == file_id))
                file_record = file_result.scalar_one_or_none()
                existing = next((f for f in staging_files if f['file_id'] == file_id), None)
                if existing:
                    existing['staged'] += staged_count
                else:
                    staging_files.append({
                        'file_id': file_id,
                        'file_name': file_record.file_name if file_record else f'æ–‡ä»¶{file_id}',
                        'staged': staged_count,
                        'domain': 'inventory'
                    })
        
        # æŸ¥è¯¢æ–‡ä»¶çŠ¶æ€å’Œå…¥åº“ç»“æœ
        logs = []
        for file_info in staging_files[:limit]:
            file_id = file_info['file_id']
            file_result = await db.execute(select(CatalogFile).where(CatalogFile.id == file_id))
            file_record = file_result.scalar_one_or_none()
            
            if file_record:
                # æŸ¥è¯¢factè¡¨å…¥åº“æ•°é‡
                imported = 0
                if file_info['domain'] == 'orders':
                    result = await db.execute(text("""
                        SELECT COUNT(*) FROM fact_orders WHERE file_id = :file_id
                    """), {"file_id": file_id})
                    imported = result.scalar() or 0
                else:
                    result = await db.execute(text("""
                        SELECT COUNT(*) FROM fact_product_metrics WHERE source_catalog_id = :file_id
                    """), {"file_id": file_id})
                    imported = result.scalar() or 0
                
                # æŸ¥è¯¢éš”ç¦»æ•°é‡ï¼ˆâ­ v4.12.1ä¿®å¤ï¼šä½¿ç”¨catalog_file_idå­—æ®µï¼‰
                result = await db.execute(text("""
                    SELECT COUNT(*) FROM data_quarantine WHERE catalog_file_id = :file_id
                """), {"file_id": file_id})
                quarantined = result.scalar() or 0
                
                logs.append({
                    'file_id': file_id,
                    'file_name': file_record.file_name,
                    'status': file_record.status or 'unknown',
                    'staged': file_info['staged'],
                    'imported': imported,
                    'quarantined': quarantined,
                    'domain': file_info['domain']
                })
        
        return {
            "success": True,
            "task_id": task_id,
            "logs": logs,
            "total": len(logs)
        }
        
    except Exception as e:
        logger.error(f"[API] æŸ¥è¯¢ä»»åŠ¡æ—¥å¿—å¤±è´¥: {e}", exc_info=True)
        return error_response(
            code=ErrorCode.DATABASE_QUERY_ERROR,
            message="æŸ¥è¯¢ä»»åŠ¡æ—¥å¿—å¤±è´¥",
            error_type=get_error_type(ErrorCode.DATABASE_QUERY_ERROR),
            detail=str(e),
            recovery_suggestion="è¯·æ£€æŸ¥æ•°æ®åº“è¿æ¥å’ŒæŸ¥è¯¢å‚æ•°ï¼Œæˆ–è”ç³»ç³»ç»Ÿç®¡ç†å‘˜",
            status_code=500
        )


@router.get("/auto-ingest/file/{file_id}/logs")
async def get_file_logs(file_id: int, limit: int = Query(50, ge=1, le=200), db: AsyncSession = Depends(get_async_db)):
    """
    é€šè¿‡æ–‡ä»¶IDè·å–ä»»åŠ¡æ—¥å¿—ï¼ˆv4.11.5æ–°å¢ï¼‰
    
    è¿”å›ï¼š
    {
        success: bool,
        file_id: int,
        logs: [
            {
                file_id: int,
                file_name: str,
                status: str,
                staged: int,
                imported: int,
                quarantined: int,
                error_message: str,
                duration_seconds: float
            }
        ]
    }
    """
    try:
        from modules.core.db import CatalogFile, StagingOrders, StagingProductMetrics, StagingInventory, DataQuarantine
        from sqlalchemy import func, text
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_result = await db.execute(select(CatalogFile).where(CatalogFile.id == file_id))
        file_record = file_result.scalar_one_or_none()
        if not file_record:
            return error_response(
                code=ErrorCode.FILE_NOT_FOUND,
                message=f"æ–‡ä»¶ä¸å­˜åœ¨: id={file_id}",
                error_type=get_error_type(ErrorCode.FILE_NOT_FOUND),
                recovery_suggestion="è¯·æ£€æŸ¥æ–‡ä»¶IDæ˜¯å¦æ­£ç¡®ï¼Œæˆ–ç¡®è®¤è¯¥æ–‡ä»¶å·²æ³¨å†Œ",
                status_code=404
            )
        
        data_domain = file_record.data_domain or "products"
        
        # æŸ¥è¯¢stagingå±‚æ•°æ®é‡
        staged = 0
        if data_domain == "orders":
            result = await db.execute(select(func.count(StagingOrders.id)).where(
                StagingOrders.file_id == file_id
            ))
            staged = result.scalar() or 0
        elif data_domain in ["products", "traffic", "analytics"]:
            result = await db.execute(select(func.count(StagingProductMetrics.id)).where(
                StagingProductMetrics.file_id == file_id
            ))
            staged = result.scalar() or 0
        elif data_domain == "inventory":
            result = await db.execute(select(func.count(StagingInventory.id)).where(
                StagingInventory.file_id == file_id
            ))
            staged = result.scalar() or 0
        
        # æŸ¥è¯¢factå±‚æ•°æ®é‡
        imported = 0
        if data_domain == "orders":
            result = await db.execute(text("""
                SELECT COUNT(*) FROM fact_orders WHERE file_id = :file_id
            """), {"file_id": file_id})
            imported = result.scalar() or 0
        else:
            result = await db.execute(text("""
                SELECT COUNT(*) FROM fact_product_metrics WHERE source_catalog_id = :file_id
            """), {"file_id": file_id})
            imported = result.scalar() or 0
        
        # æŸ¥è¯¢éš”ç¦»åŒºæ•°æ®é‡ï¼ˆâ­ v4.12.1ä¿®å¤ï¼šä½¿ç”¨catalog_file_idå­—æ®µï¼‰
        result = await db.execute(text("""
            SELECT COUNT(*) FROM data_quarantine WHERE catalog_file_id = :file_id
        """), {"file_id": file_id})
        quarantined = result.scalar() or 0
        
        # ç¡®å®šçŠ¶æ€
        status = 'ingested'
        if imported == 0 and staged > 0:
            status = 'failed'
        elif quarantined > 0:
            status = 'partial_success'
        elif staged == 0:
            status = 'skipped'
        
        logs = [{
            'file_id': file_id,
            'file_name': file_record.file_name,
            'status': status,
            'staged': staged,
            'imported': imported,
            'quarantined': quarantined,
            'domain': data_domain,
            'error_message': None,
            'duration_seconds': None
        }]
        
        return {
            "success": True,
            "file_id": file_id,
            "logs": logs,
            "total": len(logs)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[API] è·å–æ–‡ä»¶æ—¥å¿—å¤±è´¥: {e}", exc_info=True)
        return error_response(
            code=ErrorCode.DATABASE_QUERY_ERROR,
            message="è·å–æ–‡ä»¶æ—¥å¿—å¤±è´¥",
            error_type=get_error_type(ErrorCode.DATABASE_QUERY_ERROR),
            detail=str(e),
            recovery_suggestion="è¯·æ£€æŸ¥æ•°æ®åº“è¿æ¥å’ŒæŸ¥è¯¢å‚æ•°ï¼Œæˆ–è”ç³»ç³»ç»Ÿç®¡ç†å‘˜",
            status_code=500
        )


class ClearDataRequest(BaseModel):
    """æ•°æ®åº“æ¸…ç†è¯·æ±‚"""
    confirm: bool = Field(True, description="å¿…é¡»æ˜¾å¼ç¡®è®¤")


@router.post("/database/clear-all-data")
async def clear_all_data(
    request: ClearDataRequest,
    db: AsyncSession = Depends(get_async_db)
):
    """
    ä¸€é”®æ¸…ç†æ•°æ®åº“ä¸­æ‰€æœ‰ä¸šåŠ¡æ•°æ®ï¼ˆå¼€å‘é˜¶æ®µä½¿ç”¨ï¼‰
    
    æ¸…ç†èŒƒå›´ï¼š
    - æ‰€æœ‰äº‹å®è¡¨ï¼ˆfact_orders, fact_order_items, fact_order_amounts, fact_product_metricsï¼‰
    - æ‰€æœ‰æš‚å­˜è¡¨ï¼ˆstaging_orders, staging_product_metricsï¼‰
    - æ•°æ®éš”ç¦»è¡¨ï¼ˆdata_quarantineï¼‰
    - catalog_filesçŠ¶æ€é‡ç½®ä¸ºpending
    
    ä¿ç•™ï¼š
    - ç»´åº¦è¡¨ï¼ˆdim_platforms, dim_shops, dim_productsç­‰ï¼‰
    - é…ç½®è¡¨ï¼ˆfield_mapping_dictionary, field_mapping_templatesç­‰ï¼‰
    - catalog_fileså…ƒæ•°æ®ï¼ˆåªé‡ç½®çŠ¶æ€ï¼‰
    
    è­¦å‘Šï¼šæ­¤æ“ä½œä¸å¯é€†ï¼
    """
    if not request.confirm:
        return error_response(
            code=ErrorCode.DATA_VALIDATION_FAILED,
            message="å¿…é¡»æ˜¾å¼ç¡®è®¤ï¼šè¯·ä¼ é€’ confirm=true å‚æ•°",
            error_type=get_error_type(ErrorCode.DATA_VALIDATION_FAILED),
            recovery_suggestion="è¯·è®¾ç½®confirm=trueä»¥ç¡®è®¤æ‰§è¡Œæ¸…ç†æ“ä½œ",
            status_code=400
        )
    
    try:
        from sqlalchemy import text
        from backend.models.database import engine
        
        # æ¸…ç†äº‹å®è¡¨ï¼ˆä½¿ç”¨æ­£ç¡®çš„è¡¨åï¼‰
        fact_tables = [
            'fact_orders',
            'fact_order_items',
            'fact_order_amounts',  # â­ v4.18.2ï¼šå·²ä¸å†å†™å…¥æ–°æ•°æ®ï¼Œä½†ä¿ç•™æ¸…ç†é€»è¾‘ä»¥æ¸…ç†æ—§æ•°æ®
            'fact_product_metrics',
            'fact_expenses_month',
            'fact_expenses_allocated_day_shop_sku'  # ä¿®æ­£è¡¨å
        ]
        
        # æ¸…ç†æš‚å­˜è¡¨
        staging_tables = [
            'staging_orders',
            'staging_product_metrics'
        ]
        
        # æ¸…ç†æ•°æ®éš”ç¦»è¡¨
        quarantine_tables = [
            'data_quarantine'
        ]
        
        cleared_counts = {}
        total_cleared = 0
        
        async def safe_truncate_table(table_name: str, use_cascade: bool = True) -> int:
            """
            å®‰å…¨åœ°æ¸…ç†è¡¨ï¼Œå¦‚æœè¡¨ä¸å­˜åœ¨åˆ™è·³è¿‡
            
            Returns:
                æ¸…ç†çš„è¡Œæ•°
            """
            try:
                # å…ˆæ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                result = await db.execute(text("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        AND table_name = :table_name
                    )
                """), {"table_name": table_name})
                check_result = result.scalar()
                
                if not check_result:
                    logger.info(f"[DB Cleanup] è¡¨ {table_name} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                    cleared_counts[table_name] = 0
                    return 0
                
                # ç»Ÿè®¡æ¸…ç†å‰çš„è¡Œæ•°
                result = await db.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
                count_before = result.scalar() or 0
                
                if count_before > 0:
                    # ä½¿ç”¨SAVEPOINTéš”ç¦»é”™è¯¯ï¼Œé¿å…ä¸€ä¸ªè¡¨çš„é”™è¯¯å½±å“å…¶ä»–è¡¨
                    await db.execute(text("SAVEPOINT cleanup_table"))
                    try:
                        if use_cascade:
                            await db.execute(text(f"TRUNCATE TABLE {table_name} RESTART IDENTITY CASCADE"))
                        else:
                            await db.execute(text(f"TRUNCATE TABLE {table_name} RESTART IDENTITY"))
                        await db.execute(text("RELEASE SAVEPOINT cleanup_table"))
                    except Exception as e:
                        await db.execute(text("ROLLBACK TO SAVEPOINT cleanup_table"))
                        raise e
                
                cleared_counts[table_name] = count_before
                return count_before
                
            except Exception as e:
                logger.warning(f"[DB Cleanup] æ¸…ç†è¡¨ {table_name} å¤±è´¥: {e}")
                cleared_counts[table_name] = cleared_counts.get(table_name, 0)
                return 0
        
        # æ¸…ç†äº‹å®è¡¨ï¼ˆä½¿ç”¨TRUNCATEç¡®ä¿çº§è”åˆ é™¤ï¼Œå¹¶ç»Ÿè®¡æ¸…ç†æ•°é‡ï¼‰
        for table in fact_tables:
            count = await safe_truncate_table(table, use_cascade=True)
            total_cleared += count
        
        # æ¸…ç†æš‚å­˜è¡¨
        for table in staging_tables:
            count = await safe_truncate_table(table, use_cascade=True)
            total_cleared += count
        
        # æ¸…ç†æ•°æ®éš”ç¦»è¡¨
        for table in quarantine_tables:
            count = await safe_truncate_table(table, use_cascade=False)
            total_cleared += count
        
        # é‡ç½®catalog_filesçŠ¶æ€ä¸ºpendingï¼ˆé‡ç½®æ‰€æœ‰épendingçŠ¶æ€ï¼‰
        try:
            # å…ˆç»Ÿè®¡éœ€è¦é‡ç½®çš„æ–‡ä»¶æ•°ï¼ˆæ‰€æœ‰épendingçŠ¶æ€ï¼‰
            result = await db.execute(text("""
                SELECT COUNT(*) FROM catalog_files 
                WHERE status != 'pending' OR status IS NULL
            """))
            reset_count = result.scalar() or 0
            cleared_counts['catalog_files_reset'] = reset_count
            if reset_count > 0:
                # é‡ç½®æ‰€æœ‰épendingçŠ¶æ€ä¸ºpending
                await db.execute(text("""
                    UPDATE catalog_files 
                    SET status = 'pending',
                        last_processed_at = NULL
                    WHERE status != 'pending' OR status IS NULL
                """))
                total_cleared += reset_count
                logger.info(f"[DB Cleanup] é‡ç½®äº† {reset_count} ä¸ªcatalog_filesçš„çŠ¶æ€ä¸ºpending")
        except Exception as e:
            logger.warning(f"[DB Cleanup] é‡ç½®catalog_filesçŠ¶æ€å¤±è´¥: {e}")
            cleared_counts['catalog_files_reset'] = cleared_counts.get('catalog_files_reset', 0)
 
        await db.commit()

        # åˆ·æ–°ç‰©åŒ–è§†å›¾ï¼ˆæ¸…ç©ºï¼‰
        try:
            with engine.connect() as conn:
                conn = conn.execution_options(isolation_level="AUTOCOMMIT")
                conn.execute(text("REFRESH MATERIALIZED VIEW mv_product_management"))
                conn.execute(text("REFRESH MATERIALIZED VIEW mv_product_sales_trend"))
                conn.execute(text("REFRESH MATERIALIZED VIEW mv_top_products"))
            cleared_counts['materialized_views'] = 'refreshed'
        except Exception as e:
            logger.warning(f"[DB Cleanup] åˆ·æ–°ç‰©åŒ–è§†å›¾å¤±è´¥: {e}")
            cleared_counts['materialized_views'] = 'failed'
 
        # æ ¹æ®æ¸…ç†ç»“æœè¿”å›ä¸åŒçš„æ¶ˆæ¯
        if total_cleared == 0:
            message = "æ•°æ®åº“å·²ä¸ºç©ºï¼Œæ— éœ€æ¸…ç†"
            logger.info("[DB Cleanup] æ•°æ®åº“å·²ä¸ºç©ºï¼Œæ— éœ€æ¸…ç†")
        else:
            message = f"æ•°æ®åº“æ¸…ç†å®Œæˆï¼Œå…±æ¸…ç† {total_cleared} è¡Œæ•°æ®"
            logger.info(f"[DB Cleanup] æ•°æ®åº“æ¸…ç†å®Œæˆ: å…±æ¸…ç† {total_cleared} è¡Œæ•°æ®")
        
        # â­ ä¿®å¤ï¼šä½¿ç”¨æ ‡å‡†APIå“åº”æ ¼å¼
        from backend.utils.api_response import success_response
        return success_response(
            data={
                "rows_cleared": total_cleared,
                "details": cleared_counts
            },
            message=message
        )
        
    except Exception as e:
        await db.rollback()
        logger.error(f"[DB Cleanup] æ•°æ®åº“æ¸…ç†å¤±è´¥: {e}", exc_info=True)
        return error_response(
            code=ErrorCode.DATABASE_QUERY_ERROR,
            message="æ•°æ®åº“æ¸…ç†å¤±è´¥",
            error_type=get_error_type(ErrorCode.DATABASE_QUERY_ERROR),
            detail=str(e),
            recovery_suggestion="è¯·æ£€æŸ¥æ•°æ®åº“è¿æ¥å’Œæƒé™ï¼Œæˆ–è”ç³»ç³»ç»Ÿç®¡ç†å‘˜",
            status_code=500
        )

