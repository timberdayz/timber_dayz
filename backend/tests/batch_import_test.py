"""
æ‰¹é‡æ•°æ®å¯¼å…¥æµ‹è¯•å·¥å…·
"""

import asyncio
import aiohttp
import time
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import List, Dict, Any
from dataclasses import dataclass
import statistics
import os

@dataclass
class ImportTestResult:
    """å¯¼å…¥æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    test_name: str
    total_records: int
    successful_records: int
    failed_records: int
    total_time: float
    records_per_second: float
    average_response_time: float
    min_response_time: float
    max_response_time: float
    error_rate: float
    errors: List[str]
    memory_usage_mb: float

class BatchImportTester:
    """æ‰¹é‡å¯¼å…¥æµ‹è¯•å™¨"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.session = None
        self.results = []
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=300),  # 5åˆ†é’Ÿè¶…æ—¶
            connector=aiohttp.TCPConnector(limit=50, limit_per_host=20)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    def generate_test_data(self, num_records: int, data_type: str = "orders") -> List[Dict]:
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        print(f"ç”Ÿæˆ {num_records} æ¡ {data_type} æµ‹è¯•æ•°æ®...")
        
        if data_type == "orders":
            return self._generate_order_data(num_records)
        elif data_type == "products":
            return self._generate_product_data(num_records)
        elif data_type == "inventory":
            return self._generate_inventory_data(num_records)
        else:
            return self._generate_generic_data(num_records)
    
    def _generate_order_data(self, num_records: int) -> List[Dict]:
        """ç”Ÿæˆè®¢å•æµ‹è¯•æ•°æ®"""
        platforms = ["shopee", "tiktok", "amazon", "miaoshou"]
        statuses = ["pending", "paid", "shipped", "delivered", "cancelled"]
        
        data = []
        base_date = datetime.now() - timedelta(days=365)
        
        for i in range(num_records):
            order_date = base_date + timedelta(days=np.random.randint(0, 365))
            
            data.append({
                "order_id": f"ORD_{i+1:08d}",
                "platform": np.random.choice(platforms),
                "shop_id": f"shop_{np.random.randint(1, 100):03d}",
                "order_date": order_date.strftime("%Y-%m-%d"),
                "total_amount": round(np.random.uniform(10, 1000), 2),
                "currency": "USD",
                "status": np.random.choice(statuses),
                "customer_name": f"Customer_{i+1}",
                "customer_email": f"customer_{i+1}@example.com",
                "shipping_address": f"Address_{i+1}",
                "payment_method": np.random.choice(["credit_card", "paypal", "bank_transfer"]),
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat()
            })
        
        return data
    
    def _generate_product_data(self, num_records: int) -> List[Dict]:
        """ç”Ÿæˆäº§å“æµ‹è¯•æ•°æ®"""
        categories = ["Electronics", "Clothing", "Home", "Sports", "Books"]
        brands = ["BrandA", "BrandB", "BrandC", "BrandD", "BrandE"]
        
        data = []
        
        for i in range(num_records):
            data.append({
                "sku": f"SKU_{i+1:08d}",
                "product_name": f"Product_{i+1}",
                "category": np.random.choice(categories),
                "brand": np.random.choice(brands),
                "price": round(np.random.uniform(5, 500), 2),
                "cost": round(np.random.uniform(2, 250), 2),
                "weight": round(np.random.uniform(0.1, 10), 2),
                "dimensions": f"{np.random.randint(5, 50)}x{np.random.randint(5, 50)}x{np.random.randint(1, 20)}",
                "description": f"Description for product {i+1}",
                "is_active": np.random.choice([True, False]),
                "created_at": datetime.now().isoformat(),
                "updated_at": datetime.now().isoformat()
            })
        
        return data
    
    def _generate_inventory_data(self, num_records: int) -> List[Dict]:
        """ç”Ÿæˆåº“å­˜æµ‹è¯•æ•°æ®"""
        warehouses = ["WH_001", "WH_002", "WH_003", "WH_004", "WH_005"]
        
        data = []
        
        for i in range(num_records):
            data.append({
                "sku": f"SKU_{i+1:08d}",
                "warehouse": np.random.choice(warehouses),
                "quantity_on_hand": np.random.randint(0, 1000),
                "quantity_reserved": np.random.randint(0, 100),
                "quantity_available": np.random.randint(0, 900),
                "reorder_point": np.random.randint(10, 100),
                "max_stock": np.random.randint(500, 2000),
                "location": f"Location_{np.random.randint(1, 50)}",
                "last_updated": datetime.now().isoformat()
            })
        
        return data
    
    def _generate_generic_data(self, num_records: int) -> List[Dict]:
        """ç”Ÿæˆé€šç”¨æµ‹è¯•æ•°æ®"""
        data = []
        
        for i in range(num_records):
            data.append({
                "id": i + 1,
                "name": f"Record_{i+1}",
                "value": round(np.random.uniform(0, 1000), 2),
                "category": f"Category_{np.random.randint(1, 10)}",
                "status": np.random.choice(["active", "inactive", "pending"]),
                "created_at": datetime.now().isoformat()
            })
        
        return data
    
    async def test_batch_import(self, data: List[Dict], batch_size: int = 100, endpoint: str = "/api/test/batch-import") -> ImportTestResult:
        """æµ‹è¯•æ‰¹é‡å¯¼å…¥"""
        print(f"å¼€å§‹æ‰¹é‡å¯¼å…¥æµ‹è¯•: {len(data)} æ¡è®°å½•, æ‰¹æ¬¡å¤§å°: {batch_size}")
        
        start_time = time.time()
        successful_records = 0
        failed_records = 0
        response_times = []
        errors = []
        
        # åˆ†æ‰¹å¤„ç†æ•°æ®
        for i in range(0, len(data), batch_size):
            batch = data[i:i + batch_size]
            batch_start = time.time()
            
            try:
                # å‘é€æ‰¹é‡å¯¼å…¥è¯·æ±‚
                url = f"{self.base_url}{endpoint}"
                payload = {"data": batch, "batch_id": i // batch_size + 1}
                
                async with self.session.post(url, json=payload) as response:
                    response_time = time.time() - batch_start
                    response_times.append(response_time)
                    
                    if response.status < 400:
                        result = await response.json()
                        successful_records += result.get("successful_count", len(batch))
                        failed_records += result.get("failed_count", 0)
                    else:
                        failed_records += len(batch)
                        errors.append(f"HTTP {response.status}: {await response.text()}")
                
            except Exception as e:
                response_time = time.time() - batch_start
                response_times.append(response_time)
                failed_records += len(batch)
                errors.append(str(e))
            
            # æ˜¾ç¤ºè¿›åº¦
            if (i // batch_size + 1) % 10 == 0:
                progress = (i + len(batch)) / len(data) * 100
                print(f"è¿›åº¦: {progress:.1f}% ({i + len(batch)}/{len(data)})")
        
        total_time = time.time() - start_time
        
        return ImportTestResult(
            test_name="Batch Import",
            total_records=len(data),
            successful_records=successful_records,
            failed_records=failed_records,
            total_time=total_time,
            records_per_second=len(data) / total_time if total_time > 0 else 0,
            average_response_time=statistics.mean(response_times) if response_times else 0,
            min_response_time=min(response_times) if response_times else 0,
            max_response_time=max(response_times) if response_times else 0,
            error_rate=failed_records / len(data) * 100 if data else 0,
            errors=errors[:10],
            memory_usage_mb=0  # è¿™é‡Œå¯ä»¥æ·»åŠ å†…å­˜ç›‘æ§
        )
    
    async def test_individual_imports(self, data: List[Dict], max_concurrent: int = 10) -> ImportTestResult:
        """æµ‹è¯•å•ä¸ªè®°å½•å¯¼å…¥ï¼ˆå¹¶å‘ï¼‰"""
        print(f"å¼€å§‹å•ä¸ªè®°å½•å¯¼å…¥æµ‹è¯•: {len(data)} æ¡è®°å½•, æœ€å¤§å¹¶å‘: {max_concurrent}")
        
        start_time = time.time()
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def import_single_record(record, index):
            async with semaphore:
                record_start = time.time()
                try:
                    url = f"{self.base_url}/api/test/single-import"
                    async with self.session.post(url, json=record) as response:
                        response_time = time.time() - record_start
                        if response.status < 400:
                            return {"success": True, "response_time": response_time, "error": None}
                        else:
                            return {"success": False, "response_time": response_time, "error": f"HTTP {response.status}"}
                except Exception as e:
                    response_time = time.time() - record_start
                    return {"success": False, "response_time": response_time, "error": str(e)}
        
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [import_single_record(record, i) for i, record in enumerate(data)]
        
        # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        total_time = time.time() - start_time
        
        # å¤„ç†ç»“æœ
        successful = [r for r in results if isinstance(r, dict) and r.get("success", False)]
        failed = [r for r in results if isinstance(r, dict) and not r.get("success", False)]
        exceptions = [r for r in results if isinstance(r, Exception)]
        
        response_times = [r["response_time"] for r in successful + failed]
        errors = [r.get("error", "Unknown error") for r in failed] + [str(e) for e in exceptions]
        
        return ImportTestResult(
            test_name="Individual Imports",
            total_records=len(data),
            successful_records=len(successful),
            failed_records=len(failed) + len(exceptions),
            total_time=total_time,
            records_per_second=len(data) / total_time if total_time > 0 else 0,
            average_response_time=statistics.mean(response_times) if response_times else 0,
            min_response_time=min(response_times) if response_times else 0,
            max_response_time=max(response_times) if response_times else 0,
            error_rate=(len(failed) + len(exceptions)) / len(data) * 100 if data else 0,
            errors=errors[:10],
            memory_usage_mb=0
        )
    
    def print_test_result(self, result: ImportTestResult):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•åç§°: {result.test_name}")
        print(f"{'='*60}")
        print(f"æ€»è®°å½•æ•°: {result.total_records}")
        print(f"æˆåŠŸè®°å½•: {result.successful_records}")
        print(f"å¤±è´¥è®°å½•: {result.failed_records}")
        print(f"æ€»è€—æ—¶: {result.total_time:.2f}ç§’")
        print(f"æ¯ç§’å¤„ç†è®°å½•æ•°: {result.records_per_second:.2f}")
        print(f"å¹³å‡å“åº”æ—¶é—´: {result.average_response_time:.3f}ç§’")
        print(f"æœ€å°å“åº”æ—¶é—´: {result.min_response_time:.3f}ç§’")
        print(f"æœ€å¤§å“åº”æ—¶é—´: {result.max_response_time:.3f}ç§’")
        print(f"é”™è¯¯ç‡: {result.error_rate:.2f}%")
        
        if result.errors:
            print(f"\né”™è¯¯ç¤ºä¾‹:")
            for error in result.errors[:5]:
                print(f"  - {error}")
    
    def save_test_results(self, results: List[ImportTestResult], filename: str):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
        try:
            data = {
                "test_time": datetime.now().isoformat(),
                "results": [
                    {
                        "test_name": r.test_name,
                        "total_records": r.total_records,
                        "successful_records": r.successful_records,
                        "failed_records": r.failed_records,
                        "total_time": r.total_time,
                        "records_per_second": r.records_per_second,
                        "average_response_time": r.average_response_time,
                        "min_response_time": r.min_response_time,
                        "max_response_time": r.max_response_time,
                        "error_rate": r.error_rate,
                        "errors": r.errors,
                        "memory_usage_mb": r.memory_usage_mb
                    }
                    for r in results
                ]
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            print(f"\næµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            print(f"ä¿å­˜æµ‹è¯•ç»“æœå¤±è´¥: {e}")

async def run_batch_import_tests():
    """è¿è¡Œæ‰¹é‡å¯¼å…¥æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹æ‰¹é‡æ•°æ®å¯¼å…¥æµ‹è¯•")
    print("="*60)
    
    # æµ‹è¯•é…ç½®
    test_configs = [
        {"records": 1000, "batch_size": 100, "name": "å°æ‰¹é‡æµ‹è¯•"},
        {"records": 10000, "batch_size": 500, "name": "ä¸­æ‰¹é‡æµ‹è¯•"},
        {"records": 50000, "batch_size": 1000, "name": "å¤§æ‰¹é‡æµ‹è¯•"},
        {"records": 100000, "batch_size": 2000, "name": "è¶…å¤§æ‰¹é‡æµ‹è¯•"}
    ]
    
    all_results = []
    
    async with BatchImportTester() as tester:
        for config in test_configs:
            print(f"\nğŸ§ª {config['name']}: {config['records']} æ¡è®°å½•, æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
            print("-" * 60)
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            test_data = tester.generate_test_data(config["records"], "orders")
            
            # æ‰¹é‡å¯¼å…¥æµ‹è¯•
            batch_result = await tester.test_batch_import(test_data, config["batch_size"])
            tester.print_test_result(batch_result)
            all_results.append(batch_result)
            
            # å•ä¸ªå¯¼å…¥æµ‹è¯•ï¼ˆä½¿ç”¨è¾ƒå°çš„æ•°æ®é›†ï¼‰
            if config["records"] <= 10000:
                individual_data = test_data[:1000]  # é™åˆ¶ä¸º1000æ¡è®°å½•
                individual_result = await tester.test_individual_imports(individual_data, max_concurrent=20)
                tester.print_test_result(individual_result)
                all_results.append(individual_result)
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´å†è¿›è¡Œä¸‹ä¸€è½®æµ‹è¯•
            print(f"\nâ³ ç­‰å¾…10ç§’åè¿›è¡Œä¸‹ä¸€è½®æµ‹è¯•...")
            await asyncio.sleep(10)
    
    # ä¿å­˜æ‰€æœ‰æµ‹è¯•ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"temp/outputs/batch_import_test_results_{timestamp}.json"
    tester.save_test_results(all_results, filename)
    
    print(f"\nğŸ‰ æ‰¹é‡æ•°æ®å¯¼å…¥æµ‹è¯•å®Œæˆï¼")
    print(f"æ€»å…±æ‰§è¡Œäº† {len(all_results)} ä¸ªæµ‹è¯•")
    print(f"ç»“æœå·²ä¿å­˜åˆ°: {filename}")

if __name__ == "__main__":
    asyncio.run(run_batch_import_tests())
