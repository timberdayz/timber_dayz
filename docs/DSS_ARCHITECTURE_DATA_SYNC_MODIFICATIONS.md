# DSS架构数据同步流程修改总结

**日期**: 2025-02-01  
**版本**: v4.13.0  
**状态**: ✅ **修改完成**

---

## 📋 修改概述

根据DSS架构原则，移除了数据同步流程中的业务逻辑处理，只保留数据采集和存储功能。所有数据处理（字段映射、数据标准化、业务逻辑验证）应在Metabase中完成。

---

## 🔧 修改内容

### 文件1: `backend/services/data_ingestion_service.py`

#### ✅ 修改1: 移除字段映射应用环节（第227-266行）

**修改前**:
- 执行字段映射逻辑（将原始列名转换为标准字段名）
- 验证字段映射完整性
- 使用映射后的数据

**修改后**:
- 直接使用原始数据，保留所有原始列名
- 跳过字段映射，记录日志说明原因
- 向后兼容：如果提供了mappings参数，记录日志但不使用

**代码变更**:
```python
# ⭐ DSS架构：跳过字段映射，直接使用原始数据
enhanced_rows = all_rows  # 直接使用原始数据，保留所有原始列名
logger.info(f"[Ingest] [DSS] 跳过字段映射，直接使用原始数据: {len(enhanced_rows)}行，{len(original_header_columns)}个字段")
```

#### ✅ 修改2: 简化数据标准化环节（第258-262行）

**修改前**:
- 调用 `standardize_rows()` 进行数据类型转换
- 日期格式统一、数字格式标准化

**修改后**:
- 跳过数据标准化，保留原始数据格式
- 记录日志说明原因

**代码变更**:
```python
# ⭐ DSS架构：跳过数据标准化，保留原始数据
logger.info(f"[Ingest] [DSS] 跳过数据标准化，保留原始数据格式")
```

#### ✅ 修改3: 移除特殊处理环节（第264-267行）

**修改前**:
- snapshot产品数据自动补充 `metric_date`
- 从文件名提取日期并添加到数据行

**修改后**:
- 移除特殊处理，保留原始数据完整性
- 记录日志说明原因

**代码变更**:
```python
# ⭐ DSS架构：移除特殊处理，保留原始数据
logger.info(f"[Ingest] [DSS] 跳过特殊处理，保留原始数据完整性")
```

#### ✅ 修改4: 移除数据验证环节（第269-282行）

**修改前**:
- 根据数据域调用不同的验证函数（validate_orders, validate_inventory等）
- 隔离验证失败的数据到 `data_quarantine` 表
- 只处理通过验证的数据

**修改后**:
- 跳过数据验证，所有数据直接进入去重和入库流程
- 不隔离数据，`quarantined_count = 0`
- 使用所有数据，不做验证筛选

**代码变更**:
```python
# ⭐ DSS架构：跳过数据验证，直接使用所有数据
validation_result = {
    "errors": [],
    "warnings": [],
    "ok_rows": len(enhanced_rows),
    "total": len(enhanced_rows)
}
quarantined_count = 0  # DSS架构下不隔离数据
valid_rows = enhanced_rows  # 直接使用所有数据，不做验证筛选
```

#### ✅ 修改5: 更新日志信息（第286行）

**修改前**:
```python
logger.info(f"[Ingest] 数据入库开始: file_id={file_id}, 有效行数={len(valid_rows)}, 验证失败行数={len(error_rows)}")
```

**修改后**:
```python
logger.info(f"[Ingest] [DSS] 数据入库开始: file_id={file_id}, 总行数={len(valid_rows)}（DSS架构：跳过验证，所有数据入库）")
```

#### ✅ 修改6: 修复订单金额维度入库参数（第349-355行）

**修改前**:
```python
original_columns=list(mappings.keys()) if mappings else [],
field_mappings=processed_mappings,
```

**修改后**:
```python
original_columns=original_header_columns if original_header_columns else [],
field_mappings={},  # ⭐ DSS架构：不再使用字段映射
```

---

### 文件2: `backend/routers/field_mapping.py`

#### ✅ 修改1: 移除数据验证环节（第1198-1211行）

**修改前**:
- 根据数据域调用不同的验证函数
- 隔离验证失败的数据
- 只处理通过验证的数据

**修改后**:
- 跳过数据验证，所有数据直接进入去重和入库流程
- 不隔离数据
- 使用所有数据，不做验证筛选

**代码变更**:
```python
# ⭐ DSS架构：跳过数据验证，直接使用所有数据
logger.info(f"[Ingest] [DSS] 跳过数据验证，所有{len(enhanced_rows)}行数据将直接进入去重和入库流程")
validation_result = {
    "errors": [],
    "warnings": [],
    "ok_rows": len(enhanced_rows),
    "total": len(enhanced_rows)
}
quarantined_count = 0  # DSS架构下不隔离数据
valid_rows = enhanced_rows  # 直接使用所有数据，不做验证筛选
```

#### ✅ 修改2: 移除数据标准化和特殊处理（第1160-1196行）

**修改前**:
- 调用 `standardize_rows()` 进行数据标准化
- snapshot产品数据自动补充 `metric_date`

**修改后**:
- 跳过数据标准化和特殊处理
- 保留原始数据完整性

**代码变更**:
```python
# ⭐ DSS架构：跳过数据标准化，保留原始数据
logger.info(f"[Ingest] [DSS] 跳过数据标准化，保留原始数据格式")

# ⭐ DSS架构：移除特殊处理，保留原始数据
logger.info(f"[Ingest] [DSS] 跳过特殊处理，保留原始数据完整性")
```

---

## 📊 修改统计

| 文件 | 修改点 | 删除行数 | 新增行数 | 说明 |
|------|--------|---------|---------|------|
| `backend/services/data_ingestion_service.py` | 6个修改点 | ~108行 | ~20行 | 移除字段映射、数据标准化、特殊处理、数据验证 |
| `backend/routers/field_mapping.py` | 2个修改点 | ~30行 | ~10行 | 移除数据标准化、特殊处理、数据验证 |
| **总计** | **8个修改点** | **~138行** | **~30行** | **代码简化，符合DSS架构** |

---

## ✅ 修改后的完整数据同步流程

```
文件上传/采集
    ↓
文件注册到 catalog_files 表
    ↓
数据同步开始
    ↓
┌─────────────────────────────────────┐
│ 阶段0: 模板查找和配置                │
│ - 查找模板（TemplateMatcher）        │
│ - 使用模板header_row和header_columns │
│ - 检测表头变化（可选）                │
└─────────────────────────────────────┘
    ↓
┌─────────────────────────────────────┐
│ 阶段1: 文件读取和解析                │
│ - ExcelParser.read_excel(           │
│     header=template.header_row)      │
│ - 数据规范化（合并单元格还原）        │
│ - 转换为字典列表                     │
└─────────────────────────────────────┘
    ↓
┌─────────────────────────────────────┐
│ 阶段2: 补充元数据（仅元数据）        │
│ - 从file_record获取platform_code     │
│ - 从file_record获取shop_id          │
│ - 确保每行数据包含基本元数据          │
└─────────────────────────────────────┘
    ↓
┌─────────────────────────────────────┐
│ ⭐ 阶段3: 去重处理（核心）            │
│ - 计算 data_hash（SHA256）           │
│ - 批量查询已存在的哈希                │
│ - 过滤重复数据                        │
└─────────────────────────────────────┘
    ↓
┌─────────────────────────────────────┐
│ ⭐ 阶段4: 数据入库（核心）            │
│ - 准备入库数据（提取metric_date）     │
│ - 批量插入到 fact_raw_data_* 表      │
│ - ON CONFLICT 自动去重               │
│ - 更新文件状态                        │
└─────────────────────────────────────┘
    ↓
数据入库完成
    ↓
Metabase 查询和业务逻辑验证
```

---

## 🎯 关键变化

### 移除的功能

1. ❌ **字段映射应用** - 不再将原始列名转换为标准字段名
2. ❌ **数据标准化** - 不再进行数据类型转换
3. ❌ **特殊处理** - 不再自动补充 `metric_date` 等字段
4. ❌ **业务逻辑验证** - 不再验证"可用库存不能大于实际库存"等规则
5. ❌ **数据隔离** - 不再隔离验证失败的数据

### 保留的功能

1. ✅ **模板查找和配置** - 使用模板的 `header_row` 和 `header_columns`
2. ✅ **元数据补充** - 从 `file_record` 获取 `platform_code` 和 `shop_id`
3. ✅ **去重处理** - 基于 `data_hash` 的数据库级去重
4. ✅ **数据入库** - 批量插入到 B 类数据表（JSONB 格式）

---

## 🔍 模板管理与数据同步协同机制

### 新数据域处理流程

```
新数据域文件
    ↓
查找模板 → 未找到
    ↓
检查 only_with_template 参数
    ↓
    ├─ only_with_template=True（默认）
    │   ↓
    │   跳过同步，返回"无模板"
    │   提示用户：需要先创建模板
    │   ↓
    │   用户操作：
    │   1. 预览文件（选择表头行）
    │   2. 查看表头字段列表
    │   3. 保存模板（header_row + header_columns）
    │   ↓
    │   模板创建成功
    │   ↓
    │   重新触发数据同步
    │   ↓
    │   找到模板，正常同步
    │
    └─ only_with_template=False
        ↓
        使用默认配置：
        - header_row = 0
        - header_columns = 从文件读取的列名
        ↓
        数据同步（可能表头行不正确）
        ↓
        用户创建模板（保存正确的header_row）
        ↓
        后续文件使用模板，正常同步
```

### 表头更新处理流程

```
文件（新表头）
    ↓
查找模板 → 找到旧模板
    ↓
使用模板header_row读取文件
    ↓
表头匹配验证
    ↓
    ├─ 匹配率 >= 80%？
    │   ↓
    │   继续使用模板，正常同步
    │
    └─ 匹配率 < 80%？
        ↓
        检测表头变化：
        - 新增字段：X个
        - 删除字段：Y个
        - 重命名字段：Z个
        ↓
        用户选择：
        选项A: 更新模板（推荐）
            ↓
            保存新模板（新版本）
            - header_row（如果变化）
            - header_columns（新表头）
            - 旧版本自动归档
            ↓
            使用新模板同步
        ↓
        选项B: 继续使用旧模板（不推荐）
            ↓
            使用旧模板同步
            ↓
            可能数据不完整
```

---

## 📝 注意事项

### 向后兼容

1. **保留参数**:
   - `mappings` 参数仍然接受，但不使用（记录日志）
   - 验证函数导入仍然保留（但不调用）

2. **保留代码**:
   - 相关导入语句保留（向后兼容）
   - 注释说明DSS架构原则

### 日志记录

1. **所有修改都添加 `[DSS]` 标记**:
   - `[Ingest] [DSS] 跳过字段映射...`
   - `[Ingest] [DSS] 跳过数据验证...`

2. **记录跳过原因**:
   - 说明DSS架构原则
   - 说明数据处理在Metabase中完成

### 错误处理

1. **数据库约束**:
   - 数据库约束仍会捕获格式错误（如日期格式）
   - 业务逻辑错误在Metabase中处理

2. **去重机制**:
   - `ON CONFLICT DO NOTHING` 自动去重
   - 基于 `data_hash` 的唯一约束

---

## ✅ 验证结果

1. ✅ **语法检查通过**: `python -m py_compile` 无错误
2. ✅ **Linter检查通过**: 无linter错误
3. ✅ **代码逻辑正确**: 所有修改符合DSS架构原则
4. ✅ **向后兼容**: 保留参数和导入，不影响现有调用

---

## 🎯 预期效果

1. **提高数据入库率**:
   - 不会因为字段名不匹配而误隔离
   - 不会因为业务逻辑规则而误隔离

2. **符合DSS架构原则**:
   - 数据同步只做数据采集和存储
   - 不做业务逻辑判断

3. **灵活性更高**:
   - 支持任意字段名（中文/英文）
   - 支持任意数据结构（JSONB存储）

4. **性能更好**:
   - 减少验证计算开销
   - 减少数据库隔离操作

---

## 📚 相关文档

- [DSS架构指南](docs/V4_6_0_ARCHITECTURE_GUIDE.md)
- [数据隔离区标准](docs/DATA_QUARANTINE_STANDARDS.md)
- [模板管理文档](backend/services/template_matcher.py)

---

**修改完成时间**: 2025-02-01  
**状态**: ✅ **已完成，等待测试验证**

