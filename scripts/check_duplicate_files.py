#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ£€æŸ¥é‡å¤æ–‡ä»¶è„šæœ¬

æ‰¾å‡ºæ‰«ææ—¶è¢«è·³è¿‡çš„é‡å¤æ–‡ä»¶ï¼ˆåŸºäºfile_hashï¼‰ã€‚
"""

import sys
from pathlib import Path
from collections import defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from sqlalchemy import func, select
from modules.core.db import CatalogFile
from backend.models.database import SessionLocal
from modules.services.catalog_scanner import _compute_sha256
from modules.core.logger import get_logger

logger = get_logger(__name__)


def check_duplicate_files():
    """æ£€æŸ¥é‡å¤æ–‡ä»¶"""
    db = SessionLocal()
    
    try:
        # 1. ç»Ÿè®¡æ•°æ®åº“ä¸­çš„æ–‡ä»¶æ•°
        db_count = db.query(func.count(CatalogFile.id)).scalar() or 0
        print(f"\n{'='*60}")
        print(f"ğŸ“Š é‡å¤æ–‡ä»¶æ£€æŸ¥")
        print(f"{'='*60}")
        print(f"\næ•°æ®åº“ä¸­çš„æ–‡ä»¶æ•°: {db_count}")
        
        # 2. æ‰«ææ–‡ä»¶ç³»ç»Ÿï¼Œæ‰¾å‡ºæ‰€æœ‰æ–‡ä»¶
        base_dir = Path("data/raw")
        all_files = []
        seen_hashes = set()
        duplicate_files = []
        
        if base_dir.exists():
            # æ‰«ææ‰€æœ‰å¹´ä»½ç›®å½•
            for year_dir in base_dir.iterdir():
                if not year_dir.is_dir() or not year_dir.name.isdigit():
                    continue
                
                for file_path in year_dir.rglob("*.xlsx"):
                    if file_path.suffix == '.json':
                        continue
                    
                    # è®¡ç®—æ–‡ä»¶hash
                    try:
                        file_hash = _compute_sha256(file_path)
                        all_files.append((file_path, file_hash))
                        
                        # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²å­˜åœ¨
                        existing = db.execute(
                            select(CatalogFile).where(CatalogFile.file_hash == file_hash)
                        ).scalar_one_or_none()
                        
                        if existing:
                            if file_hash in seen_hashes:
                                duplicate_files.append((file_path, file_hash, existing.id))
                            else:
                                seen_hashes.add(file_hash)
                    except Exception as e:
                        logger.warning(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
        
        # 3. ç»Ÿè®¡ç»“æœ
        total_files = len(all_files)
        unique_files = len(seen_hashes)
        duplicate_count = total_files - unique_files
        
        print(f"\næ‰«æç»“æœ:")
        print(f"  - æ€»æ–‡ä»¶æ•°: {total_files}")
        print(f"  - å”¯ä¸€æ–‡ä»¶æ•°ï¼ˆåŸºäºhashï¼‰: {unique_files}")
        print(f"  - é‡å¤æ–‡ä»¶æ•°: {duplicate_count}")
        
        if duplicate_count > 0:
            print(f"\nâš ï¸  å‘ç° {duplicate_count} ä¸ªé‡å¤æ–‡ä»¶ï¼ˆç›¸åŒå†…å®¹ï¼‰")
            print(f"   è¿™äº›æ–‡ä»¶åœ¨æ‰«ææ—¶ä¼šè¢«æ›´æ–°è€Œä¸æ˜¯æ–°å¢")
            print(f"\né‡å¤æ–‡ä»¶åˆ—è¡¨ï¼ˆå‰10ä¸ªï¼‰:")
            for i, (file_path, file_hash, existing_id) in enumerate(duplicate_files[:10]):
                print(f"  {i+1}. {file_path.name}")
                print(f"     Hash: {file_hash[:16]}...")
                print(f"     å·²å­˜åœ¨è®°å½•ID: {existing_id}")
        else:
            print(f"\nâœ… æ²¡æœ‰å‘ç°é‡å¤æ–‡ä»¶")
        
        # 4. å¯¹æ¯”åˆ†æ
        print(f"\n{'='*60}")
        print(f"ğŸ” å¯¹æ¯”åˆ†æ")
        print(f"{'='*60}")
        print(f"\næ‰«æå‘ç°æ–‡ä»¶æ•°: {total_files}")
        print(f"æ•°æ®åº“ä¸­çš„æ–‡ä»¶æ•°: {db_count}")
        print(f"å·®å¼‚: {total_files - db_count}ä¸ªæ–‡ä»¶")
        
        if total_files - db_count == duplicate_count:
            print(f"\nâœ… å·®å¼‚åŸå› å·²ç¡®è®¤ï¼š{duplicate_count}ä¸ªé‡å¤æ–‡ä»¶")
            print(f"   è¿™æ˜¯æ­£å¸¸è¡Œä¸ºï¼šåŸºäºfile_hashå»é‡ï¼Œé‡å¤æ–‡ä»¶åªæ›´æ–°ä¸æ–°å¢")
        elif total_files - db_count > duplicate_count:
            print(f"\nâš ï¸  å·®å¼‚å¤§äºé‡å¤æ–‡ä»¶æ•°")
            print(f"   å¯èƒ½è¿˜æœ‰å…¶ä»–åŸå› å¯¼è‡´æ–‡ä»¶æœªæ³¨å†Œ")
        
        print(f"\n{'='*60}\n")
        
    except Exception as e:
        logger.error(f"æ£€æŸ¥å¤±è´¥: {e}", exc_info=True)
        print(f"\nâŒ æ£€æŸ¥å¤±è´¥: {e}")
    finally:
        db.close()


if __name__ == "__main__":
    check_duplicate_files()

