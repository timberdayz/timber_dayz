#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ£€æŸ¥pendingæ–‡ä»¶çš„é‡å¤æƒ…å†µ

ç»Ÿè®¡ï¼š
1. æ•°æ®åº“ä¸­pendingçŠ¶æ€çš„æ–‡ä»¶æ€»æ•°
2. source='data/raw'çš„pendingæ–‡ä»¶æ•°
3. æ˜¯å¦æœ‰é‡å¤çš„file_hash
4. å®é™…æ‰«æåˆ°çš„æ–‡ä»¶æ•°ï¼ˆé€šè¿‡æ–‡ä»¶ç³»ç»Ÿï¼‰
"""

import sys
from pathlib import Path
from collections import Counter

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from sqlalchemy import func
from modules.core.db import CatalogFile
from backend.models.database import SessionLocal
from modules.core.logger import get_logger
from modules.core.path_manager import get_data_raw_dir

logger = get_logger(__name__)


def check_duplicate_pending_files():
    """æ£€æŸ¥pendingæ–‡ä»¶çš„é‡å¤æƒ…å†µ"""
    db = SessionLocal()
    
    try:
        print(f"\n{'='*80}")
        print(f"ğŸ“Š æ£€æŸ¥pendingæ–‡ä»¶é‡å¤æƒ…å†µ")
        print(f"{'='*80}\n")
        
        # 1. ç»Ÿè®¡æ•°æ®åº“ä¸­pendingçŠ¶æ€çš„æ–‡ä»¶æ€»æ•°
        total_pending = db.query(func.count(CatalogFile.id)).filter(
            CatalogFile.status == 'pending'
        ).scalar() or 0
        
        print(f"ğŸ“‹ æ•°æ®åº“ä¸­pendingçŠ¶æ€çš„æ–‡ä»¶æ€»æ•°: {total_pending}")
        
        # 2. ç»Ÿè®¡source='data/raw'çš„pendingæ–‡ä»¶æ•°
        data_raw_pending = db.query(func.count(CatalogFile.id)).filter(
            CatalogFile.status == 'pending',
            CatalogFile.source == 'data/raw'
        ).scalar() or 0
        
        print(f"ğŸ“‹ source='data/raw'çš„pendingæ–‡ä»¶æ•°: {data_raw_pending}")
        
        # 3. æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„file_hash
        pending_files = db.query(CatalogFile.file_hash, CatalogFile.file_path).filter(
            CatalogFile.status == 'pending',
            CatalogFile.source == 'data/raw',
            CatalogFile.file_hash.isnot(None)
        ).all()
        
        hash_counts = Counter(hash_val for hash_val, _ in pending_files if hash_val)
        duplicate_hashes = {h: c for h, c in hash_counts.items() if c > 1}
        
        print(f"\nğŸ“Š file_hashé‡å¤æƒ…å†µ:")
        print(f"   å”¯ä¸€hashæ•°: {len(hash_counts)}")
        print(f"   é‡å¤hashæ•°: {len(duplicate_hashes)}")
        if duplicate_hashes:
            print(f"   é‡å¤hashè¯¦æƒ…ï¼ˆå‰10ä¸ªï¼‰:")
            for hash_val, count in list(duplicate_hashes.items())[:10]:
                print(f"     - {hash_val[:16]}... : {count}ä¸ªæ–‡ä»¶")
        
        # 4. ç»Ÿè®¡å®é™…æ‰«æåˆ°çš„æ–‡ä»¶æ•°ï¼ˆé€šè¿‡æ–‡ä»¶ç³»ç»Ÿï¼‰
        scan_dir = get_data_raw_dir()
        scanned_files = []
        for year_dir in scan_dir.iterdir():
            if year_dir.is_dir() and year_dir.name.isdigit() and len(year_dir.name) == 4:
                for file_path in year_dir.rglob("*.*"):
                    if file_path.suffix.lower() in {'.csv', '.xlsx', '.xls'}:
                        if file_path.suffix != '.json':  # è·³è¿‡.meta.json
                            scanned_files.append(file_path)
        
        print(f"\nğŸ“Š æ–‡ä»¶ç³»ç»Ÿæ‰«æç»“æœ:")
        print(f"   æ‰«æåˆ°çš„æ–‡ä»¶æ•°: {len(scanned_files)}")
        
        # 5. ç»Ÿè®¡å®é™…å­˜åœ¨çš„pendingæ–‡ä»¶æ•°
        existing_count = sum(1 for _, file_path in pending_files if file_path and Path(file_path).exists())
        
        print(f"\n{'='*80}")
        print(f"ğŸ“Š æ€»ç»“")
        print(f"{'='*80}")
        print(f"æ•°æ®åº“ä¸­pendingæ€»æ•°: {total_pending}")
        print(f"source='data/raw'çš„pendingæ•°: {data_raw_pending}")
        print(f"å®é™…å­˜åœ¨çš„pendingæ–‡ä»¶æ•°: {existing_count}")
        print(f"æ–‡ä»¶ç³»ç»Ÿæ‰«æåˆ°çš„æ–‡ä»¶æ•°: {len(scanned_files)}")
        print(f"é‡å¤hashæ•°: {len(duplicate_hashes)}")
        print(f"{'='*80}\n")
        
    except Exception as e:
        logger.error(f"æ£€æŸ¥å¤±è´¥: {e}", exc_info=True)
        print(f"\nâŒ æ£€æŸ¥å¤±è´¥: {e}\n")
    finally:
        db.close()


if __name__ == "__main__":
    check_duplicate_pending_files()

