#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ£€æŸ¥Shopeeè®¢å•æ–‡ä»¶çš„é‡å¤è®°å½•

æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦æœ‰é‡å¤çš„file_hashæˆ–file_path
"""

import sys
from pathlib import Path
from collections import defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from sqlalchemy import func
from modules.core.db import CatalogFile
from backend.models.database import SessionLocal

db = SessionLocal()

try:
    print(f"\n{'='*80}")
    print(f"ğŸ“Š æ£€æŸ¥Shopeeè®¢å•æ–‡ä»¶é‡å¤è®°å½•")
    print(f"{'='*80}\n")
    
    # 1. æŸ¥è¯¢æ‰€æœ‰Shopeeè®¢å•æ–‡ä»¶
    shopee_orders = db.query(CatalogFile).filter(
        CatalogFile.platform_code == 'shopee',
        CatalogFile.data_domain == 'orders'
    ).all()
    
    print(f"ğŸ“‹ æ•°æ®åº“ä¸­Shopeeè®¢å•æ–‡ä»¶æ€»æ•°: {len(shopee_orders)}ä¸ª\n")
    
    # 2. æ£€æŸ¥file_hashé‡å¤
    hash_counts = defaultdict(list)
    for file_record in shopee_orders:
        if file_record.file_hash:
            hash_counts[file_record.file_hash].append(file_record)
    
    duplicate_hashes = {h: files for h, files in hash_counts.items() if len(files) > 1}
    
    print(f"ğŸ“Š file_hashé‡å¤æƒ…å†µ:")
    print(f"   å”¯ä¸€hashæ•°: {len(hash_counts)}")
    print(f"   é‡å¤hashæ•°: {len(duplicate_hashes)}")
    if duplicate_hashes:
        print(f"   é‡å¤hashè¯¦æƒ…:")
        for hash_val, files in list(duplicate_hashes.items())[:5]:
            print(f"     Hash: {hash_val[:16]}... ({len(files)}ä¸ªæ–‡ä»¶)")
            for file_record in files:
                print(f"       - ID: {file_record.id}, æ–‡ä»¶å: {file_record.file_name}, çŠ¶æ€: {file_record.status}")
    print()
    
    # 3. æ£€æŸ¥file_pathé‡å¤
    path_counts = defaultdict(list)
    for file_record in shopee_orders:
        if file_record.file_path:
            path_counts[file_record.file_path].append(file_record)
    
    duplicate_paths = {p: files for p, files in path_counts.items() if len(files) > 1}
    
    print(f"ğŸ“Š file_pathé‡å¤æƒ…å†µ:")
    print(f"   å”¯ä¸€è·¯å¾„æ•°: {len(path_counts)}")
    print(f"   é‡å¤è·¯å¾„æ•°: {len(duplicate_paths)}")
    if duplicate_paths:
        print(f"   é‡å¤è·¯å¾„è¯¦æƒ…:")
        for path, files in list(duplicate_paths.items())[:5]:
            print(f"     è·¯å¾„: {path}")
            for file_record in files:
                print(f"       - ID: {file_record.id}, æ–‡ä»¶å: {file_record.file_name}, çŠ¶æ€: {file_record.status}, hash: {file_record.file_hash[:16] if file_record.file_hash else 'None'}...")
    print()
    
    # 4. æ£€æŸ¥file_nameé‡å¤
    name_counts = defaultdict(list)
    for file_record in shopee_orders:
        if file_record.file_name:
            name_counts[file_record.file_name].append(file_record)
    
    duplicate_names = {n: files for n, files in name_counts.items() if len(files) > 1}
    
    print(f"ğŸ“Š file_nameé‡å¤æƒ…å†µ:")
    print(f"   å”¯ä¸€æ–‡ä»¶åæ•°: {len(name_counts)}")
    print(f"   é‡å¤æ–‡ä»¶åæ•°: {len(duplicate_names)}")
    if duplicate_names:
        print(f"   é‡å¤æ–‡ä»¶åè¯¦æƒ…:")
        for name, files in list(duplicate_names.items())[:10]:
            print(f"     æ–‡ä»¶å: {name} ({len(files)}ä¸ªè®°å½•)")
            for file_record in files:
                print(f"       - ID: {file_record.id}, çŠ¶æ€: {file_record.status}, è·¯å¾„: {file_record.file_path}")
    print()
    
    # 5. æ€»ç»“
    print(f"{'='*80}")
    print(f"ğŸ“Š æ€»ç»“")
    print(f"{'='*80}")
    print(f"æ•°æ®åº“ä¸­Shopeeè®¢å•æ–‡ä»¶æ€»æ•°: {len(shopee_orders)}ä¸ª")
    print(f"å”¯ä¸€file_hashæ•°: {len(hash_counts)}ä¸ª")
    print(f"å”¯ä¸€file_pathæ•°: {len(path_counts)}ä¸ª")
    print(f"å”¯ä¸€file_nameæ•°: {len(name_counts)}ä¸ª")
    print(f"é‡å¤hashæ•°: {len(duplicate_hashes)}ä¸ª")
    print(f"é‡å¤è·¯å¾„æ•°: {len(duplicate_paths)}ä¸ª")
    print(f"é‡å¤æ–‡ä»¶åæ•°: {len(duplicate_names)}ä¸ª")
    print(f"{'='*80}\n")
    
except Exception as e:
    print(f"\nâŒ æ£€æŸ¥å¤±è´¥: {e}\n")
    import traceback
    traceback.print_exc()
finally:
    db.close()

