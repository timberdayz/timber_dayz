#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯Šæ–­åº“å­˜æ–‡ä»¶é‡å¤é—®é¢˜

æ£€æŸ¥ï¼š
1. æ–‡ä»¶ç³»ç»Ÿä¸­æœ‰å¤šå°‘åº“å­˜æ–‡ä»¶
2. æ•°æ®åº“ä¸­æœ‰å¤šå°‘åº“å­˜æ–‡ä»¶è®°å½•
3. ä¸ºä»€ä¹ˆä¼šæœ‰é‡å¤è®°å½•
"""

import sys
from pathlib import Path
from collections import defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from sqlalchemy import func
from modules.core.db import CatalogFile
from backend.models.database import SessionLocal
from modules.core.path_manager import get_data_raw_dir
from modules.services.catalog_scanner import _compute_sha256

db = SessionLocal()

try:
    print(f"\n{'='*80}")
    print(f"ğŸ“Š åº“å­˜æ–‡ä»¶é‡å¤é—®é¢˜è¯Šæ–­")
    print(f"{'='*80}\n")
    
    # 1. æ£€æŸ¥æ–‡ä»¶ç³»ç»Ÿä¸­çš„åº“å­˜æ–‡ä»¶
    print(f"ğŸ“‹ æ–‡ä»¶ç³»ç»Ÿä¸­çš„åº“å­˜æ–‡ä»¶:")
    scan_dir = get_data_raw_dir()
    inventory_files = []
    
    for year_dir in scan_dir.iterdir():
        if year_dir.is_dir() and year_dir.name.isdigit() and len(year_dir.name) == 4:
            for file_path in year_dir.rglob("*.*"):
                if file_path.suffix.lower() in {'.csv', '.xlsx', '.xls'}:
                    if file_path.suffix != '.json':
                        file_name_lower = file_path.name.lower()
                        if 'inventory' in file_name_lower or 'snapshot' in file_name_lower:
                            inventory_files.append(file_path)
    
    print(f"   æ‰«æåˆ°çš„æ–‡ä»¶æ•°: {len(inventory_files)}ä¸ª")
    if inventory_files:
        print(f"   æ–‡ä»¶åˆ—è¡¨:")
        for file_path in sorted(inventory_files):
            print(f"     - {file_path.name}")
    print()
    
    # 2. æ£€æŸ¥æ•°æ®åº“ä¸­çš„åº“å­˜æ–‡ä»¶è®°å½•
    print(f"ğŸ“‹ æ•°æ®åº“ä¸­çš„åº“å­˜æ–‡ä»¶è®°å½•:")
    db_inventory = db.query(CatalogFile).filter(
        CatalogFile.data_domain == 'inventory'
    ).all()
    
    print(f"   æ•°æ®åº“è®°å½•æ•°: {len(db_inventory)}ä¸ª")
    print()
    
    # 3. æ£€æŸ¥file_hashé‡å¤æƒ…å†µ
    print(f"ğŸ“Š file_hashé‡å¤æƒ…å†µ:")
    hash_counts = defaultdict(list)
    for file_record in db_inventory:
        if file_record.file_hash:
            hash_counts[file_record.file_hash].append(file_record)
    
    duplicate_hashes = {h: files for h, files in hash_counts.items() if len(files) > 1}
    unique_hashes = {h: files for h, files in hash_counts.items() if len(files) == 1}
    
    print(f"   å”¯ä¸€hashæ•°: {len(hash_counts)}")
    print(f"   é‡å¤hashæ•°: {len(duplicate_hashes)}")
    print(f"   å”¯ä¸€hashè®°å½•æ•°: {len(unique_hashes)}")
    print()
    
    # 4. æ£€æŸ¥file_pathé‡å¤æƒ…å†µ
    # v4.18.0: ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œä¸æ•°æ®åº“å­˜å‚¨æ ¼å¼ä¸€è‡´
    print(f"ğŸ“Š file_pathé‡å¤æƒ…å†µ:")
    path_counts = defaultdict(list)
    for file_record in db_inventory:
        if file_record.file_path:
            # ç›´æ¥ä½¿ç”¨æ•°æ®åº“ä¸­å­˜å‚¨çš„è·¯å¾„æ ¼å¼ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
            path_counts[file_record.file_path].append(file_record)
    
    duplicate_paths = {p: files for p, files in path_counts.items() if len(files) > 1}
    
    print(f"   å”¯ä¸€è·¯å¾„æ•°: {len(path_counts)}")
    print(f"   é‡å¤è·¯å¾„æ•°: {len(duplicate_paths)}")
    if duplicate_paths:
        print(f"   é‡å¤è·¯å¾„è¯¦æƒ…:")
        for path, files in list(duplicate_paths.items())[:10]:
            print(f"     è·¯å¾„: {path}")
            for file_record in files:
                print(f"       - ID: {file_record.id}, æ–‡ä»¶å: {file_record.file_name}")
                print(f"         çŠ¶æ€: {file_record.status}, shop_id: {file_record.shop_id}")
                print(f"         hash: {file_record.file_hash[:16] if file_record.file_hash else 'None'}...")
    print()
    
    # 5. æ£€æŸ¥file_nameé‡å¤æƒ…å†µ
    print(f"ğŸ“Š file_nameé‡å¤æƒ…å†µ:")
    name_counts = defaultdict(list)
    for file_record in db_inventory:
        if file_record.file_name:
            name_counts[file_record.file_name].append(file_record)
    
    duplicate_names = {n: files for n, files in name_counts.items() if len(files) > 1}
    
    print(f"   å”¯ä¸€æ–‡ä»¶åæ•°: {len(name_counts)}")
    print(f"   é‡å¤æ–‡ä»¶åæ•°: {len(duplicate_names)}")
    if duplicate_names:
        print(f"   é‡å¤æ–‡ä»¶åè¯¦æƒ…:")
        for name, files in sorted(duplicate_names.items()):
            print(f"     æ–‡ä»¶å: {name} ({len(files)}ä¸ªè®°å½•)")
            for file_record in files:
                print(f"       - ID: {file_record.id}, çŠ¶æ€: {file_record.status}")
                print(f"         è·¯å¾„: {file_record.file_path}")
                print(f"         shop_id: {file_record.shop_id}, platform: {file_record.platform_code}")
                print(f"         hash: {file_record.file_hash[:16] if file_record.file_hash else 'None'}...")
    print()
    
    # 6. æ£€æŸ¥hashè®¡ç®—æ–¹å¼å·®å¼‚
    print(f"ğŸ“Š file_hashè®¡ç®—æ–¹å¼æ£€æŸ¥:")
    print(f"   æ£€æŸ¥æ—§è®°å½•å’Œæ–°è®°å½•çš„hashæ˜¯å¦ä¸åŒ...")
    
    # å¯¹äºé‡å¤çš„æ–‡ä»¶åï¼Œæ£€æŸ¥å®ƒä»¬çš„hashæ˜¯å¦ä¸åŒ
    for name, files in list(duplicate_names.items())[:5]:
        if len(files) > 1:
            print(f"\n   æ–‡ä»¶å: {name}")
            # æ‰¾åˆ°å®é™…æ–‡ä»¶
            actual_file = None
            for file_path in inventory_files:
                if file_path.name == name:
                    actual_file = file_path
                    break
            
            if actual_file:
                # ä½¿ç”¨æ–°æ–¹å¼è®¡ç®—hashï¼ˆåŒ…å«shop_idå’Œplatform_codeï¼‰
                new_hash = _compute_sha256(
                    actual_file,
                    shop_id='none',  # miaoshouåº“å­˜æ–‡ä»¶çš„shop_idåº”è¯¥æ˜¯'none'
                    platform_code='miaoshou'
                )
                print(f"     å®é™…æ–‡ä»¶è·¯å¾„: {actual_file}")
                print(f"     æ–°hashï¼ˆshop_id='none', platform='miaoshou'ï¼‰: {new_hash[:16]}...")
                
                for file_record in files:
                    print(f"      è®°å½•ID {file_record.id}:")
                    print(f"        æ—§hash: {file_record.file_hash[:16] if file_record.file_hash else 'None'}...")
                    print(f"        shop_id: {file_record.shop_id}")
                    print(f"        platform: {file_record.platform_code}")
                    if file_record.file_hash == new_hash:
                        print(f"        âœ… hashåŒ¹é…")
                    else:
                        print(f"        âŒ hashä¸åŒ¹é…ï¼ˆå¯èƒ½æ˜¯æ—§è®¡ç®—æ–¹å¼ï¼‰")
    
    # 7. æ€»ç»“
    print(f"\n{'='*80}")
    print(f"ğŸ“Š è¯Šæ–­æ€»ç»“")
    print(f"{'='*80}")
    print(f"æ–‡ä»¶ç³»ç»Ÿä¸­åº“å­˜æ–‡ä»¶: {len(inventory_files)}ä¸ª")
    print(f"æ•°æ®åº“ä¸­åº“å­˜æ–‡ä»¶è®°å½•: {len(db_inventory)}ä¸ª")
    print(f"å”¯ä¸€file_hashæ•°: {len(hash_counts)}ä¸ª")
    print(f"å”¯ä¸€file_pathæ•°: {len(path_counts)}ä¸ª")
    print(f"å”¯ä¸€file_nameæ•°: {len(name_counts)}ä¸ª")
    print(f"é‡å¤hashæ•°: {len(duplicate_hashes)}ä¸ª")
    print(f"é‡å¤è·¯å¾„æ•°: {len(duplicate_paths)}ä¸ª")
    print(f"é‡å¤æ–‡ä»¶åæ•°: {len(duplicate_names)}ä¸ª")
    print()
    
    if len(duplicate_names) > 0:
        print(f"ğŸ” é—®é¢˜åˆ†æ:")
        print(f"   âš ï¸  å‘ç°é‡å¤è®°å½•ï¼š{len(duplicate_names)}ä¸ªæ–‡ä»¶åæœ‰å¤šä¸ªè®°å½•")
        print(f"   ğŸ’¡ å¯èƒ½åŸå› ï¼š")
        print(f"      1. file_hashè®¡ç®—æ–¹å¼æ”¹å˜ï¼ˆv4.17.3ä¿®å¤åï¼ŒhashåŒ…å«shop_idå’Œplatform_codeï¼‰")
        print(f"      2. æ—§è®°å½•çš„hashä¸åŒ…å«shop_idå’Œplatform_codeï¼Œæ–°è®°å½•çš„hashåŒ…å«")
        print(f"      3. å¯¼è‡´åŒä¸€ä¸ªæ–‡ä»¶è¢«æ³¨å†Œä¸ºå¤šæ¡è®°å½•")
        print(f"   ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š")
        print(f"      1. é‡æ–°è®¡ç®—æ‰€æœ‰æ—§è®°å½•çš„file_hashï¼ˆä½¿ç”¨æ–°çš„è®¡ç®—æ–¹å¼ï¼‰")
        print(f"      2. æˆ–è€…æ¸…ç†é‡å¤è®°å½•ï¼Œåªä¿ç•™æœ€æ–°çš„è®°å½•")
        print(f"      3. æˆ–è€…ä¿®æ”¹å»é‡é€»è¾‘ï¼ŒåŸºäºfile_pathè€Œä¸æ˜¯file_hash")
    
    print(f"{'='*80}\n")
    
except Exception as e:
    print(f"\nâŒ è¯Šæ–­å¤±è´¥: {e}\n")
    import traceback
    traceback.print_exc()
finally:
    db.close()

